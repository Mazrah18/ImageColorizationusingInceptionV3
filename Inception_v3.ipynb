{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5HFEWJDWERE"
      },
      "source": [
        "# **IMPORT LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vNil4o_ZV7Ad"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.layers import Conv2D, UpSampling2D, Input, Reshape, concatenate\n",
        "from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
        "from skimage.transform import resize\n",
        "from skimage.io import imsave\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaLkmIOwWM9O"
      },
      "source": [
        "# **SETTING UP THE MODEL**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "civMRgvaWTCp"
      },
      "source": [
        "### **CHECKING FOR GPU ACCESS** : Comment out the code below if you do not have tensorflow-gpu installed in your system. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IsIPxbzsV7Ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m physical_devices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mlist_physical_devices(\u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(physical_devices)\n\u001b[1;32m----> 8\u001b[0m tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mset_memory_growth(physical_devices[\u001b[39m0\u001b[39;49m], \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Check if GPU is available\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mif\u001b[39;00m physical_devices:\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set GPU device\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "# GPU memory growth\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "print(physical_devices)\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "# Check if GPU is available\n",
        "if physical_devices:\n",
        "    print(\"GPU is available. Running on GPU.\")\n",
        "else:\n",
        "    print(\"GPU is not available. Running on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W140QaYoWY-V"
      },
      "source": [
        "### **Helper Functions to assist the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "clxsFOm6XBUy"
      },
      "outputs": [],
      "source": [
        "\n",
        "#CONVERTING Images TO Arrays\n",
        "def img_to_array(img, data_format=None, dtype=None):\n",
        "    if data_format is None:\n",
        "        data_format = 'channels_last'\n",
        "    if dtype is None:\n",
        "        dtype = np.float32\n",
        "\n",
        "    x = np.asarray(img, dtype=dtype)\n",
        "\n",
        "    if data_format == 'channels_first':\n",
        "        x = x.transpose(2, 0, 1)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HdYNjfSUl7GN"
      },
      "outputs": [],
      "source": [
        "#CONVERTING ARRAYS TO IMAGES\n",
        "def array_to_img(x, data_format=None, scale=True, dtype=None):\n",
        "    if scale:\n",
        "        x = x.clip(0, 255)\n",
        "    x = x.astype('uint8')\n",
        "    return Image.fromarray(x, mode='RGB')\n",
        "\n",
        "\n",
        "#LOADING THE IMAGES\n",
        "def load_img(path, grayscale=False, color_mode='rgb', target_size=None, interpolation='nearest'):\n",
        "    return Image.open(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XKeGZjt5XDZ_"
      },
      "outputs": [],
      "source": [
        "# Get images\n",
        "X = []\n",
        "image_dir = 'true_images/'\n",
        "num_images = 100  # Number of images to load\n",
        "target_image_size = (256, 256)\n",
        "\n",
        "\n",
        "#Training for only 100 images as it is faster and gives good results. Taking more images  may lead to out of memory errors or more time to train the model.\n",
        "for i, filename in enumerate(os.listdir(image_dir)):\n",
        "    if i >= num_images:\n",
        "        break\n",
        "    img = load_img(os.path.join(image_dir, filename))\n",
        "    img = img.resize(target_image_size)  # Resize image\n",
        "    X.append(img_to_array(img))\n",
        "\n",
        "#USED helper function to convert image into arrays.\n",
        "X = np.array(X, dtype=float)\n",
        "Xtrain = 1.0 / 255 * X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_NGEw7SXHK6"
      },
      "source": [
        "# **Define the Model along with the custom Architecture for coloring**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_WqwesRPV7Af"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load weights\n",
        "inception = InceptionV3(weights='imagenet', include_top=True)\n",
        "\n",
        "embed_input = Input(shape=(1000,))\n",
        "\n",
        "# Encoder\n",
        "encoder_input = Input(shape=(256, 256, 1,))\n",
        "encoder_output = Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(encoder_input)\n",
        "encoder_output = Conv2D(128, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n",
        "encoder_output = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "encoder_output = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n",
        "\n",
        "# Fusion\n",
        "fusion_output = RepeatVector(32 * 32)(embed_input)\n",
        "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
        "fusion_output = concatenate([encoder_output, fusion_output], axis=3)\n",
        "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n",
        "\n",
        "# Decoder\n",
        "decoder_output = Conv2D(128, (3, 3), activation='relu', padding='same')(fusion_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(64, (3, 3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "decoder_output = Conv2D(32, (3, 3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(16, (3, 3), activation='relu', padding='same')(decoder_output)\n",
        "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
        "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
        "\n",
        "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjOAn0Pqb4eA"
      },
      "source": [
        "# **Training the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJWbspa5Y_C0"
      },
      "source": [
        "### **More Helper functions to assist in training the model.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vLFp1njjV7Af"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_inception_embedding(grayscaled_rgb):\n",
        "    grayscaled_rgb_resized = []\n",
        "    for i in grayscaled_rgb:\n",
        "        i = resize(i, (299, 299, 3), mode='constant')\n",
        "        grayscaled_rgb_resized.append(i)\n",
        "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
        "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
        "    embed = inception.predict(grayscaled_rgb_resized)\n",
        "    return embed\n",
        "\n",
        "\n",
        "\n",
        "# Image transformer\n",
        "datagen = ImageDataGenerator(\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    rotation_range=20,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# Generate training data\n",
        "batch_size = 10\n",
        "\n",
        "def image_a_b_gen(batch_size):\n",
        "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
        "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
        "        embed = create_inception_embedding(grayscaled_rgb)\n",
        "        lab_batch = rgb2lab(batch)\n",
        "        X_batch = lab_batch[:, :, :, 0]\n",
        "        X_batch = X_batch.reshape(X_batch.shape + (1,))\n",
        "        Y_batch = lab_batch[:, :, :, 1:] / 128\n",
        "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "tDoB7eVhV7Ag",
        "outputId": "0525fb91-f0b6-4c47-bb35-2ad250c04a60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 276ms/step\n",
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 625ms/step\n",
            "1/1 [==============================] - 0s 339ms/steploss: 0.01\n",
            "1/1 [==============================] - 1s 727ms/step\n",
            "1/1 [==============================] - 0s 301ms/steploss: 0.30\n",
            "1/1 [==============================] - 1s 519ms/steploss: 0.23\n",
            "1/1 [==============================] - 0s 276ms/step\n",
            "1/1 [==============================] - 1s 748ms/step\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Train model and saving the weights for every 10 epochs.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model\u001b[39m.\u001b[39;49mfit(image_a_b_gen(batch_size), epochs\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m, steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[checkpoint])\n",
            "File \u001b[1;32mc:\\Users\\parth\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\parth\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\parth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\parth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\parth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\parth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\parth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\parth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\parth\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 273ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Define the checkpoint callback\n",
        "checkpoint = ModelCheckpoint(\"weights/model_weights_{epoch:02d}.h5\", save_weights_only=True, period=10)\n",
        "\n",
        "# Train model and saving the weights for every 10 epochs.\n",
        "model.compile(optimizer='rmsprop', loss='mse')\n",
        "model.fit(image_a_b_gen(batch_size), epochs=500, steps_per_epoch=10, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZlitJUiZYix"
      },
      "source": [
        "# **Testing Part**: Takes grayscale images as input as tries to color the image using the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iiQ64QI1YPHp"
      },
      "outputs": [],
      "source": [
        "# Load black-and-white images\n",
        "bw_images_dir = 'bw_images/'\n",
        "color_me = []\n",
        "\n",
        "\n",
        "#TAken a small sample of images to test as taking all the images leads to out of memory error.\n",
        "# num_images = 100  #CAN CHANGE THIS NUMBER BUT may lead to memory error\n",
        "\n",
        "for filename in os.listdir(bw_images_dir)[:num_images]:\n",
        "    img = Image.open(os.path.join(bw_images_dir, filename))\n",
        "    img = img.resize((256, 256))  # Resize image to target size\n",
        "    color_me.append(np.array(img))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q9pjsVVuaJjy"
      },
      "outputs": [],
      "source": [
        "color_me = np.array(color_me, dtype=float)\n",
        "color_me = color_me / 255.0  # Normalize to the range [0, 1]\n",
        "\n",
        "# Convert grayscale images to RGB\n",
        "color_me_rgb = np.stack((color_me,) * 3, axis=-1)\n",
        "\n",
        "# Convert RGB images to LAB color space\n",
        "color_me_lab = rgb2lab(color_me_rgb)\n",
        "\n",
        "# Separate L, AB channels\n",
        "color_me_l = color_me_lab[:, :, :, 0]\n",
        "color_me_ab = color_me_lab[:, :, :, 1:]\n",
        "\n",
        "# Reshape L channel for model input\n",
        "color_me_l = color_me_l.reshape(color_me_l.shape + (1,))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31pj4AUDaL28",
        "outputId": "862c9254-5945-40c3-d0ba-d632e249de7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 2s 577ms/step\n",
            "4/4 [==============================] - 3s 763ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\parth\\anaconda3\\lib\\site-packages\\skimage\\_shared\\utils.py:394: UserWarning: Color data out of range: Z < 0 in 505 pixels\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Generate output\n",
        "color_me_embed = create_inception_embedding(color_me_rgb[:num_images])  # Limit to the same number of images\n",
        "\n",
        "# Load the highest trained weights to use\n",
        "model.load_weights(\"weights\\model_weights_500.h5\")\n",
        "\n",
        "# Predict colorized images\n",
        "output = model.predict([color_me_l, color_me_embed][:num_images])\n",
        "output = output * 128.0\n",
        "\n",
        "# Merge L channel with predicted AB channels\n",
        "colorized_images_lab = np.concatenate([color_me_l, output], axis=3)\n",
        "\n",
        "# Convert LAB images to RGB\n",
        "colorized_images_rgb = lab2rgb(colorized_images_lab)\n",
        "\n",
        "# Convert floating-point values to the range [0, 255] and change data type to uint8\n",
        "colorized_images_rgb = (colorized_images_rgb * 255.0).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D9gJK3lJV7Ag"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Output directory path\n",
        "output_dir = 'Colored_results'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Save colorized images\n",
        "for i in range(len(colorized_images_rgb)):\n",
        "    imsave(os.path.join(output_dir, \"result_%d.png\" % (i + 1)), colorized_images_rgb[i])\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "VIEW THE RESULTS IN THE 'Colored_results' directory"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
